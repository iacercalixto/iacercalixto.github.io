-
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2017
  img: acl2017
  title: "Doubly-Attentive Decoder for Multi-modal Neural Machine Translation"
  authors: "<strong>Iacer Calixto</strong>, Qun Liu, Nick Campbell"
  doc-url: http://aclweb.org/anthology/P17-1175
  booktitle: "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"
  abstract: >
      We introduce a Multi-modal Neural Machine Translation model in which a doubly-attentive decoder naturally incorporates spatial visual features obtained using pre-trained convolutional neural networks, bridging the gap between image description and translation. Our decoder learns to attend to source-language words and parts of an image independently by means of two separate attention mechanisms as it generates words in the target language. We find that our model can efficiently exploit not just back-translated in-domain multi-modal data but also large general-domain text-only MT corpora. We also report state-of-the-art results on the Multi30k data set.
  bibtex: >
    @InProceedings{CalixtoACL2017,
      author = 	    "Calixto, Iacer and Liu, Qun and Campbell, Nick",
      title = 	    "Doubly-Attentive Decoder for Multi-modal Neural Machine Translation",
      booktitle =   "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      year = 	    "2017",
      publisher =   "Association for Computational Linguistics",
      pages = 	    "1913--1924",
      location =    "Vancouver, Canada",
      doi = 	    "10.18653/v1/P17-1175",
      url = 	    "http://www.aclweb.org/anthology/P17-1175"
    }
-
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2017
  img: emnlp2017
  title: "Incorporating Global Visual Features into Attention-Based Neural Machine Translation"
  authors: "<strong>Iacer Calixto</strong>, Qun Liu"
  doc-url: http://aclweb.org/anthology/D17-1105
  booktitle: "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
  abstract: >
      We   introduce   multi-modal, attention-based Neural Machine Translation (NMT) models which incorporate visual features into  different  parts  of  both  the  encoder and  the  decoder.   Global  image  features are  extracted  using  a  pre-trained  convolutional  neural  network  and  are  incorporated (i) as words in the source sentence, (ii) to  initialise  the  encoder  hidden  state, and (iii) as  additional  data  to  initialise the  decoder  hidden  state.   In  our  experiments,  we  evaluate  translations  into  English and German, how different strategies to incorporate global image features compare  and  which  ones  perform  best.   We also study the impact that adding synthetic multi-modal, multilingual data brings and find  that  the  additional  data  have  a  positive  impact  on  multi-modal  models.   We report new state-of-the-art results and our best models also significantly improve on a comparable Phrase-Based Statistical MT (PBSMT) model trained on the Multi30k data set according to all metrics evaluated. To the best of our knowledge, it is the first time  a  purely  neural  model  significantly improves over a PBSMT model on all metrics evaluated on this data set.
  bibtex: >
    @InProceedings{calixto-liu:2017:EMNLP2017,
      author    = {Calixto, Iacer  and  Liu, Qun},
      title     = {Incorporating Global Visual Features into Attention-based Neural Machine Translation.},
      booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
      month     = {September},
      year      = {2017},
      address   = {Copenhagen, Denmark},
      publisher = {Association for Computational Linguistics},
      pages     = {992--1003},
      abstract  = {We introduce multi-modal, attention-based neural machine translation (NMT)
            models which incorporate visual features into different parts of both the
            encoder and the decoder. Global image features are extracted using a
            pre-trained convolutional neural network and are incorporated (i) as words in
            the source sentence, (ii) to initialise the encoder hidden state, and (iii) as
            additional data to initialise the decoder hidden state. In our experiments, we
            evaluate translations into English and German, how different strategies to
            incorporate global image features compare and which ones perform best. We also
            study the impact that adding synthetic multi-modal, multilingual data brings
            and find that the additional data have a positive impact on multi-modal NMT
            models. We report new state-of-the-art results and our best models also
            significantly improve on a comparable phrase-based Statistical MT (PBSMT) model
            trained on the Multi30k data set according to all metrics evaluated. To the
            best of our knowledge, it is the first time a purely neural model significantly
            improves over a PBSMT model on all metrics evaluated on this data set.},
      url       = {https://www.aclweb.org/anthology/D17-1105}
    }
-
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2017
  img: eacl2017
  title: "Using Images to Improve Machine-Translating E-Commerce Product Listings"
  authors: "<strong>Iacer Calixto</strong>, Daniel Stein, Evgeny Matusov, Pintu Lohar, Sheila Castilho, Andy Way"
  doc-url: http://aclweb.org/anthology/E17-2101
  booktitle: "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers"
  abstract: >
      In this paper we study the impact of using images to machine-translate user-generated ecommerce product listings. We study how a multi-modal Neural Machine Translation (NMT) model compares to two text-only approaches: a conventional state-of-the-art attentional NMT and a Statistical Machine Translation (SMT) model. User-generated product listings often do not constitute grammatical or well-formed sentences. More often than not, they consist of the juxtaposition of short phrases or keywords. We train our models end-to-end as well as use text-only and multimodal NMT models for re-ranking n-best lists generated by an SMT model. We qualitatively evaluate our user-generated training data also analyse how adding synthetic data impacts the results. We evaluate our models quantitatively using BLEU and TER and find that (i) additional synthetic data has a general positive impact on text-only and multi-modal NMT models, and that (ii) using a multi-modal NMT model for re-ranking n-best lists improves TER significantly across different nbest list sizes.
  bibtex: >
    @InProceedings{Calixto2017EACL,
      author    = {Calixto, Iacer  and  Stein, Daniel  and  Matusov, Evgeny  and  Lohar, Pintu  and  Castilho, Sheila  and  Way, Andy},
      title     = {Using Images to Improve Machine-Translating E-Commerce Product Listings.},
      booktitle = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
      month     = {April},
      year      = {2017},
      address   = {Valencia, Spain},
      publisher = {Association for Computational Linguistics},
      pages     = {637--643},
      url       = {http://www.aclweb.org/anthology/E17-2101}
    }
-
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2017
  img: ranlp2017
  title: "Sentence-Level Multilingual Multi-modal Embedding for Natural Language Processing"
  authors: "<strong>Iacer Calixto</strong>, Qun Liu"
  doc-url: http://www.acl-bg.org/proceedings/2017/RANLP%202017/pdf/RANLP020.pdf
  booktitle: "Proceedings of Recent Advances in Natural Language Processing"
  abstract: >
      We propose a novel discriminative ranking model that learns embeddings from multilingual and multi-modal data, meaning that our model can take advantage of images and descriptions in multiple language to improve embedding quality. To that end, we introduce an objective function that uses pairwise ranking adapted to the case of three or more input sources. We compare our model against different baselines, and evaluate the robustness of our embeddings on imageâ€“sentence ranking (ISR), semantic textual similarity (STS), and neural machine translation (NMT). We find that the additional multilingual signals lead to improvements on all three tasks, and we highlight that our model can be used to consistently improve the adequacy of translations generated with NMT models when re-ranking n-best lists.
  bibtex: >
    @InProceedings{Calixto2017RANLP,
      author    = {Calixto, Iacer and Liu, Qun},
      title     = {{Sentence-Level Multilingual Multi-modal Embedding for Natural Language Processing}},
      booktitle = {Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017},
      month     = {September},
      year      = {2017},
      address   = {Varna, Bulgaria},
      pages     = {139--148},
      url       = {https://doi.org/10.26615/978-954-452-049-6_020}
    }
-
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2017
  img: vl2017
  title: "Human Evaluation of Multi-modal Neural Machine Translation: A Case-Study on E-Commerce Listing Titles"
  authors: "<strong>Iacer Calixto</strong>, Daniel Stein, Evgeny Matusov, Sheila Castilho, Andy Way"
  doc-url: http://www.aclweb.org/anthology/W17-2004
  booktitle: "Proceedings of the 6th Workshop on Vision and Language"
  abstract: >
      In this paper, we study how humans perceive the use of images as an additional knowledge source to machine-translate user-generated product listings in an e-commerce company. We conduct a human evaluation where we assess how a multi-modal neural machine translation (NMT) model compares to two text-only approaches: a conventional state-of-the-art attention-based NMT and a phrase-based statistical machine translation (PBSMT) model. We evaluate translations obtained with different systems and also discuss the data set of user-generated product listings, which in our case comprises both product listings and associated images. We found that humans preferred translations obtained with a PBSMT system to both text-only and multi-modal NMT over 56% of the time. Nonetheless, human evaluators ranked translations from a multi-modal NMT model as better than those of a text-only NMT over 88% of the time, which suggests that images do help NMT in this use-case.
  bibtex: >
    @InProceedings{Calixto2017VL,
      author    = {Calixto, Iacer  and  Stein, Daniel  and  Matusov, Evgeny  and  Castilho, Sheila  and  Way, Andy},
      title     = {Human Evaluation of Multi-modal Neural Machine Translation: A Case-Study on E-Commerce Listing Titles},
      booktitle = {Proceedings of the Sixth Workshop on Vision and Language},
      month     = {April},
      year      = {2017},
      address   = {Valencia, Spain},
      publisher = {Association for Computational Linguistics},
      pages     = {31--37},
      url       = {http://www.aclweb.org/anthology/W17-2004}
    }
-
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2017
  img: inlg2017
  title: "Linguistic realisation as machine translation: Comparing different MT models for AMR-to-text generation"
  authors: "Thiago Castro Ferreira, <strong>Iacer Calixto</strong>, Sander Wubben and Emiel Krahmer"
  doc-url: http://www.aclweb.org/anthology/W17-3501
  booktitle: "Proceedings of The 10th International Natural Language Generation conference"
  abstract: >
      In this paper, we study AMR-to-text generation, framing it as a translation task and comparing two different MT approaches (Phrase-based and Neural MT). We systematically study the effects of 3 AMR preprocessing steps (Delexicalisation, Compression, and Linearisation) applied before the MT phase. Our results show that preprocessing indeed helps, although the benefits differ for the two MT models. The implementation of the models are publicly available.
  bibtex: >
    @InProceedings{CastroFerreira2017INLG,
      author = 	"Castro Ferreira, Thiago and Calixto, Iacer and Wubben, Sander and Krahmer, Emiel",
      title = 	"Linguistic realisation as machine translation: Comparing different MT models for AMR-to-text generation",
      booktitle = 	"Proceedings of the 10th International Conference on Natural Language Generation",
      year = 	"2017",
      publisher = 	"Association for Computational Linguistics",
      pages = 	"1--10",
      location = 	"Santiago de Compostela, Spain",
      url = 	"http://aclweb.org/anthology/W17-3501"
    }
-
  layout: paper
  paper-type: article
  selected: y
  year: 2017
  img: pbml2017
  title: "Is Neural Machine Translation the New State-of-the-Art?"
  authors: "Sheila Castilho, Joss Moorkens, Federico Gaspari, <strong>Iacer Calixto</strong>, John Tinsley, Andy Way"
  doc-url: https://ufal.mff.cuni.cz/pbml/108/art-castilho-moorkens-gaspari-tinsley-calixto-way.pdf
  journal: "The Prague Bulletin of Mathematical Linguistics"
  abstract: >
      This paper discusses neural machine translation (NMT), a new paradigm in the MT field, comparing the quality of NMT systems with statistical MT by describing three studies using automatic and human evaluation methods. Automatic evaluation results presented for NMT are very promising, however human evaluations show mixed results. We report increases in fluency but inconsistent results for adequacy and post-editing effort. NMT undoubtedly represents a step forward for the MT field, but one that the community should be careful not to oversell.
  bibtex: >
    @article{castilho-moorkens-gaspari-tinsley-calixto-way:2017,
     journal = {The Prague Bulletin of Mathematical Linguistics},
     title = {{Is Neural Machine Translation the New State of the Art?}},
     author = {Sheila Castilho and Joss Moorkens and Federico Gaspari and Iacer Calixto and John Tinsley and Andy Way},
     year = {2017},
     month = {June},
     volume = {108},
     pages = {109--120},
     doi = {10.1515/pralin-2017-0013},
     issn = {0032-6585},
     url = {https://ufal.mff.cuni.cz/pbml/108/art-castilho-moorkens-gaspari-tinsley-calixto-way.pdf}
    }

