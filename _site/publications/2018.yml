-
  layout: article
  paper-type: article
  selected: y
  year: "2019 (article accepted)"
  img: mtjournal-2019
  title: "An Error Analysis for Image-Based Multi-Modal Neural Machine Translation"
  authors: "<strong>Iacer Calixto</strong>, Qun Liu"
  journal: "Machine Translation Journal: Special Issue in Human Factors in Neural Machine Translation"
  abstract: >
    In this article, we conduct an extensive qualitative error analysis of different multi-modal neural machine translation (MNMT) models which integrate visual features into different parts of both the encoder and the decoder.
    We investigate how different training data availability scenarios impact different models, and analyse translations from German into English: in a first scenario, (i) there is only a small training data set of parallel sentence pairs with images, or (ii) in addition to that there are additional image descriptions in the target language with images, which are incorporated with back-translation (Sennrich et al., 2016).
    We analyse two different types of MNMT models, that use global and local image features: the latter encode an image globally, i.e. there is one feature vector computed for an entire image, whereas the former encode spatial information, i.e. there are multiple feature vectors, each encoding different portions of the image.
    We conduct an extensive error analysis of translations generated by different multi-modal NMT models as well as text-only baselines, where we study how multi-modal models compare when translating both visual and non-visual terms.
    In general, we find that the additional multi-modal signals consistently improve translations, even more so when using simpler multi-modal NMT models that use global visual features, and also that not only translations of terms with a strong visual connotation are improved, but almost all kinds of errors decreased by using (some) multi-modal models and more training data.
  bibtex: >
    @Article{Calixto2017MTJournal,
      author = 	    "Calixto, Iacer and Liu, Qun and Campbell, Nick",
      title = 	    "An Error Analysis for Image-Based Multi-Modal Neural Machine Translation",
      journal =     "Machine Translation Journal: Special Issue in Human Factors in Neural Machine Translation",
      year = 	    "2018 (under review)",
      publisher =   "Springer"
    }
-
  layout: article
  paper-type: article
  selected: y
  year: "2018"
  img: latent-visual-cues-2018
  doc-url: https://arxiv.org/abs/1811.00357
  title: "Latent Visual Cues for Neural Machine Translation"
  authors: "<strong>Iacer Calixto</strong>, Miguel Rios, Wilker Aziz"
  journal: "arXiv preprint: 1811.00357"
  abstract: >
    In this work, we propose to model the interaction between visual and textual features for multi-modal neural machine translation through a latent variable model.
    This latent variable can be seen as a stochastic embedding and it is used in the target-language decoder and also to predict image features.
    Importantly, even though in our model formulation we capture correlations between visual and textual features, we do not require that images be available at test time.
    We show that our latent variable MMT formulation improves considerably over strong baselines, including the multi-task learning approach of Elliott and Kadar (2017) and the conditional variational auto-encoder approach of Toyama et al. (2016).
    Finally, in an ablation study we show that (i) predicting image features in addition to only conditioning on them and (ii) imposing a constraint on the minimum amount of information encoded in the latent variable slightly improved translations.
  bibtex: >
    @Article{Calixto2018LatentVisualCues,
      author = 	    "Iacer Calixto and Miguel Rios and Wilker Aziz",
      title = 	    "Latent Visual Cues for Neural Machine Translation",
      journal =     "arXiv preprint: 1811.00357",
      year = 	    "2018",
      publisher =   "Arxiv"
    }

