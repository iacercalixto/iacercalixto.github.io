-
  priority: 10000
  layout: article
  paper-type: article
  selected: y
  year: 2022
  img: visualsemrepl2022
  title: "Endowing Language Models with Multimodal Knowledge Graph Representations"
  authors: "Ningyuan (Teresa) Huang, Yash R. Deshpande, Yibo Liu, Houda Alberts, Kyunghyun Cho, Clara Vania, <strong>Iacer Calixto</strong>"
  venues: long,preprint
  journal: arXiv.org
  journal-url: https://arxiv.org/pdf/2206.13163
  bibtex: >
    @article{huang2022endowing,
    title={Endowing Language Models with Multimodal Knowledge Graph Representations},
    author={Huang, Ningyuan (Teresa) and Deshpande, Yash R. and Liu, Yibo and Alberts, Houda and Cho, Kyunghyun and Vania, Clara and Calixto, Iacer},
    journal={arXiv preprint arXiv:2206.13163},
    year={2022}
    }
  bibtex_file: /assets/bib/huang2022endowing.bib
-
  priority: 99
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2022
  img: euphemism2022
  title: "Detecting Euphemisms with Literal Descriptions and Visual Imagery"
  authors: "Ílker Kesen, Aykut Erdem, Erkut Erdem, <strong>Iacer Calixto</strong>"
  venues: short,workshop
  doc-url: https://arxiv.org/abs/2211.04576
  booktitle: 3rd Workshop on Figurative Language Processing at EMNLP
  bibtex: >
    @article{kesen2022detecting,
      title={Detecting Euphemisms with Literal Descriptions and Visual Imagery},
      author={Kesen, {\.I}lker and Erdem, Aykut and Erdem, Erkut and Calixto, Iacer},
      journal={arXiv preprint arXiv:2211.04576},
      year={2022}
    }
  bibtex_file: /assets/bib/euphemism2022.bib
-
  priority: 100
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2022
  img: valse2021
  title: "VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena"
  authors: "Letitia Parcalabescu, Michele Cafagna, Lilitta Muradjan, Anette Frank, <strong>Iacer Calixto</strong>, Albert Gatt"
  venues: long,conference
  doc-url: https://arxiv.org/abs/2112.07566
  booktitle: ACL
  bibtex: >
    @article{parcalabescu2021valse,
      title={VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena},
      author={Parcalabescu, Letitia and Cafagna, Michele and Muradjan, Lilitta and Frank, Anette and Calixto, Iacer and Gatt, Albert},
      journal={arXiv preprint arXiv:2112.07566},
      year={2021}
    }
  bibtex_file: /assets/bib/valse2021.bib
-
  priority: 200
  layout: article
  paper-type: article
  selected: y
  year: 2021
  img: hashtagseg2021
  title: "Zero-shot hashtag segmentation for multilingual sentiment analysis"
  authors: "Ruan Chaves Rodrigues, Marcelo Akira Inuzuka, Juliana Resplande Sant'Anna Gomes, Acquila Santos Rocha, <strong>Iacer Calixto</strong>, Hugo Alexandre Dantas do Nascimento"
  venues: long,preprint
  journal: arXiv.org
  journal-url: https://arxiv.org/abs/2112.03213
  bibtex: >
    @article{rodrigues2021zero,
    title={Zero-shot hashtag segmentation for multilingual sentiment analysis},
    author={Rodrigues, Ruan Chaves and Inuzuka, Marcelo Akira and Gomes, Juliana Resplande Sant'Anna and Rocha, Acquila Santos and Calixto, Iacer and Nascimento, Hugo Alexandre Dantas do},
    journal={arXiv preprint arXiv:2112.03213},
    year={2021}
    }
  bibtex_file: /assets/bib/hashtagseg2021.bib
-
  priority: 300
  layout: article
  paper-type: article
  selected: y
  year: 2022
  img: jair2021
  title: "Neural Natural Language Generation: A Survey on Multilinguality, Multimodality, Controllability and Learning"
  authors: "Erkut Erdem, Menekse Kuyu, Semih Yagcioglu, Anette Frank, Letiția Pârcălăbescu, Barbara Plank, Andrii Babii, Oleksii Turuta, Aykut Erdem, <strong>Iacer Calixto</strong>, Elena Lloret, Elena-Simona Apostol, Ciprian-Octavian Truică, Branislava Šandrih, Albert Gatt, Sanda Martinčić-Ipšic, Gábor Berend, Gražina Korvel"
  venues: journal,long
  journal: Journal of Artificial Intelligence Research
-
  priority: 400
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2021
  img: wikibert-2021
  title: "Wikipedia Entities as *Rendezvous* across Languages: Grounding Multilingual Language Models by Predicting Wikipedia Hyperlinks"
  authors: "<strong>Iacer Calixto</strong>, Alessandro Raganato, Tommaso Pasini"
  doc-url: /pdfs/Calixto-Raganato-Pasini_NAACL2021.pdf
  booktitle: "NAACL"
  venues: conference,short
  abstract: >
      Masked language models have quickly become the *de facto* standard when processing text. Recently, several approaches have been proposed to further enrich word representations with external knowledge sources such as knowledge graphs. However, these models are devised and evaluated in a monolingual setting only. In this work, we propose a language-independent entity prediction task as an intermediate training procedure to ground word representations on entity semantics and bridge the gap across different languages by means of a shared vocabulary of entities. We show that our approach effectively injects new lexical-semantic knowledge into neural models, improving their performance on different semantic tasks in the zero-shot crosslingual setting. As an additional advantage, our intermediate training does not require any supplementary input, allowing our models to be applied to new datasets right away. In our experiments, we use Wikipedia articles in up to 100 languages and already observe consistent gains compared to strong baselines when predicting entities using only the English Wikipedia. Further adding extra languages lead to improvements in most tasks up to a certain point, but overall we found it non-trivial to scale improvements in model transferability by training on ever increasing amounts of Wikipedia languages.
  bibtex_file: /assets/bib/naacl2021.bib
  bibtex: >
    @inproceedings{calixto-raganato-pasini-2021,
    title = "Wikipedia Entities as Rendezvous across Languages: Grounding Multilingual Language Models by Predicting Wikipedia Hyperlinks",
    author = "Iacer Calixto and Alessandro Raganato and Tommaso Pasini",
    booktitle = "Proceedings of the 2021 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://iacercalixto.github.io/pdfs/Calixto-Raganato-Pasini_NAACL2021.pdf",
    pages = "",
    abstract = "Masked language models have quickly become the de facto standard when processing text. Recently, several approaches have been proposed to further enrich word representations with external knowledge sources such as knowledge graphs. However, these models are devised and evaluated in a monolingual setting only. In this work, we propose a language-independent entity prediction task as an intermediate training procedure to ground word representations on entity semantics and bridge the gap across different languages by means of a shared vocabulary of entities. We show that our approach effectively injects new lexical-semantic knowledge into neural models, improving their performance on different semantic tasks in the zero-shot crosslingual setting. As an additional advantage, our intermediate training does not require any supplementary input, allowing our models to be applied to new datasets right away. In our experiments, we use Wikipedia articles in up to 100 languages and already observe consistent gains compared to strong baselines when predicting entities using only the English Wikipedia. Further adding extra languages lead to improvements in most tasks up to a certain point, but overall we found it non-trivial to scale improvements in model transferability by training on ever increasing amounts of Wikipedia languages.",
    }

-
  priority: 10000
  layout: paper
  paper-type: inproceedings
  selected: n
  year: 2012
  img: sbpo2011
  title: "A fuzzy set approach to estimating OD matrices in congested Brazilian traffic networks"
  authors: "Leslie Foulds, Hugo do Nascimento, <strong>Iacer Calixto</strong>, Bryon Hall, Humberto Longo"
  booktitle: XLIII SBPC
  [//]: #  booktitle: Anais do XLIII Encontro da Sociedade Brasileira de Pesquisa Operacional.
  doc-url: http://www.din.uem.br/sbpo/sbpo2011/pdf/86440.pdf
  venues: conference,long
  abstract: >
     In this paper we describe a new method for estimating origin–destination (OD) matrices for congested urban traffic networks. It is assumed that the input data includes incomplete, imprecise estimates of: link counts, trip table entries, numbers of departures from origins and numbers of arrivals at destinations. The method is based on a sequence of fuzzy linear programs and is designed especially for the particular characteristics of medium-to-large Brazilian cities. When there doesn’t exist a user–equilibrium traffic assignment that corresponds to the input data, the method provides a range of traffic assignments and their related OD matrices, within the spectrum of (i) satisfaction of the inputted estimates and (ii) a user–equilibrium assignment. The method has been tested on two numerical examples, one proposed by the authors and the other a classic one from the literature.
  bibtex_file: /assets/bib/sbpo2011.bib
  bibtex: >
    @inproceedings{Foulds2011SBPO, 
    author = {Foulds, L. R.; do Nascimento, H. A. D.; Calixto, I.; Hall, B. Longo, H.}, 
    title = {A fuzzy set approach to estimating OD matrices in congested Brazilian traffic networks}, 
    booktitle = {Anais do XLIII, SBPO.}, 
    year = {2011}, 
    address = {Ubatuba, SP}, 
    url = {http://www.din.uem.br/sbpo/sbpo2011/pdf/86440.pdf}
    }
      
-
  priority: 10000
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2012
  img: vl2012
  title: "Images as context in Statistical Machine Translation"
  authors: "<strong>Iacer Calixto</strong>, Lucia Specia, Teófilo de Campos"
  booktitle: The Workshop on Vision and Language
  doc-url: https://pdfs.semanticscholar.org/0673/e9003c048f143915bad01024f812957ac068.pdf
  poster: http://personal.ee.surrey.ac.uk/T.Decampos/VisionLanguage/CalixtoDecamposSpecia_VLNet2012_poster.pdf
  venues: workshop,short
  bibtex_file: /assets/bib/vl2012.bib
  bibtex: >
    @inproceedings{Calixto2012VL, 
    author = {Iacer Calixto and Teo E. deCampos and Lucia Specia}, 
    title = {Images as Context in Statistical Machine Translation }, 
    booktitle = {Second Annual Meeting of the EPSRC Network on Vision & Language}, 
    year = {2012}, 
    address = {Sheffield}, 
    url = {http://www.ee.surrey.ac.uk/CVSSP/Publications/papers/Calixto-VL-2012.pdf}
    }

-
  priority: 10000
  layout: article
  paper-type: article
  selected: n
  year: 2013
  img: ejor2013
  title: "A fuzzy set based approach to origin-destination matrix estimation in urban traffic networks with imprecise data"
  authors: "Les Foulds, Hugo do Nascimento, <strong>Iacer Calixto</strong>, Bryon Hall, Humberto Longo"
  [//]: # journal: The European Journal of Operations Research
  venues: journal,long
  journal: EJOR
  journal-url: https://www.sciencedirect.com/science/article/pii/S0377221713004116
  abstract: >
    An important issue in the management of urban traffic networks is the estimation of origin–destination (O–D) matrices whose entries represent the travel demands of network users. We discuss the challenges of O–D matrix estimation with incomplete, imprecise data. We propose a fuzzy set-based approach that utilises successive linear approximation. The fuzzy sets used have triangular membership functions that are easy to interpret and enable straightforward calibration of the parameters that weight the discrepancy between observed data and those predicted by the proposed approach. The method is potentially useful when prior O–D matrix entry estimates are unavailable or scarce, requiring trip generation information on origin departures and/or destination arrivals, leading to multiple modelling alternatives. The method may also be useful when there is no O–D matrix that can be user-optimally assigned to the network to reproduce observed link counts exactly. The method has been tested on some numerical examples from the literature and the results compare favourably with the results of earlier methods. It has also been successfully used to estimate O–D matrices for a practical urban traffic network in Brazil.
  bibtex_file: /assets/bib/ejor2013.bib
  bibtex: >
    @article{Foulds2013EJOR,
    title = "A fuzzy set-based approach to origin–destination matrix estimation in urban traffic networks with imprecise data",
    journal = "European Journal of Operational Research",
    volume = "231",
    number = "1",
    pages = "190 - 201",
    year = "2013",
    issn = "0377-2217",
    doi = "https://doi.org/10.1016/j.ejor.2013.05.012",
    url = "http://www.sciencedirect.com/science/article/pii/S0377221713004116",
    author = "Les R. Foulds and Hugo A.D. do Nascimento and Iacer C.A.C. Calixto and Bryon R. Hall and Humberto Longo",
    keywords = "Traffic, O–D matrix estimation, Successive linear approximation, Linear programming, Fuzzy sets"
    }
-
  priority: 10000
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2014
  img: wmt2014
  title: "Target-Centric Features for Translation Quality Estimation"
  authors: "Chris Hokamp, <strong>Iacer Calixto</strong>, Joachim Wagner, Jian Zhang"
  [//]: # booktitle: The Workshop on Statistical Machine Translation
  booktitle: WMT
  doc-url: https://www.aclweb.org/anthology/W14-3341
  venues: sharedtask
  abstract: >
    We describe the DCU-MIXED and DCUSVR submissions to the WMT-14 Quality Estimation task 1.1, predicting sentencelevel perceived post-editing effort. Feature design focuses on target-side features as we hypothesise that the source side has little effect on the quality of human translations, which are included in task 1.1 of this year’s WMT Quality Estimation shared task. We experiment with features of the QuEst framework, features of our past work, and three novel feature sets. Despite these efforts, our two systems perform poorly in the competition. Follow up experiments indicate that the poor performance is due to improperly optimised parameters.
  bibtex_file: /assets/bib/wmt2014.bib
  bibtex: >
    @InProceedings{Hokamp2014WMT,
      author    = {Hokamp, Chris  and  Calixto, Iacer  and  Wagner, Joachim  and  Zhang, Jian},
      title     = {Target-Centric Features for Translation Quality Estimation},
      booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
      month     = {June},
      year      = {2014},
      address   = {Baltimore, Maryland, USA},
      publisher = {Association for Computational Linguistics},
      pages     = {329--334},
      url       = {http://www.aclweb.org/anthology/W14-3341}
    }
-
  priority: 10000
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2014
  img: wmt2014a
  title: "Experiments in Medical Translation Shared Task at WMT 2014"
  authors: "Jian Zhang, Xiaofeng Wu, <strong>Iacer Calixto</strong>, Ali Vahid, Andy Way, Qun Liu"
  [//]: # booktitle: Proceedings of the ACL 2014 Ninth Workshop of Statistical Machine Translation
  booktitle: WMT
  doc-url: http://www.aclweb.org/anthology/W14-3332
  venues: sharedtask,long
  abstract: >
    This paper describes Dublin City University’s (DCU) submission to the WMT 2014 Medical Summary task. We report our results on the test data set in the French to English translation direction. We also report statistics collected from the corpora used to train our translation system. We conducted our experiment on the Moses 1.0 phrase-based translation system framework. We performed a variety of experiments on translation models, reordering models, operation sequence model and language model. We also experimented with data selection and removal the length constraint for phrase-pair extraction.
  bibtex_file: /assets/bib/wmt2014a.bib
  bibtex: >
    @InProceedings{Zhang2014WMT,
      author    = {Jian Zhang, Xiaofeng Wu, <strong>Iacer Calixto</strong>, Ali Hosseinzadeh Vahid, Xiaojun Zhang, Andy Way, Qun Liu},
      title     = {Experiments in Medical Translation Shared Task at WMT 2014},
      booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
      month     = {June},
      year      = {2014},
      address   = {Baltimore, Maryland, USA},
      publisher = {Association for Computational Linguistics},
      pages     = {260--265},
      url       = {http://www.aclweb.org/anthology/W14-3332}
    }
-
  priority: 10000
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2015
  img: ranlp2015
  title: "Automatic Text Simplification for Spanish: Comparative Evaluation of Various Simplification Strategies"
  authors: "Sanja Stajner, <strong>Iacer Calixto</strong>, Horacio Saggion"
  [//]: # booktitle: Proceedings of Recent Advances in Natural Language Processing
  booktitle: RANLP
  doc-url: http://www.aclweb.org/anthology/R15-1080
  venues: conference,long
  abstract: >
    In this paper, we explore statistical machine translation (SMT) approaches to automatic text simplification (ATS) for Spanish. First, we compare the performances of the standard phrase-based (PB) and hierarchical (HIERO) SMT models in this specific task. In both cases, we build two models, one using the TS corpus with “light” simplifications and the other using the TS corpus with “heavy” simplifications. Next, we compare the two best systems with the state-of-the-art text simplification system for Spanish (Simplext). Our results, based on an extensive human evaluation, show that the SMT-based systems perform equally as well as, or better than, Simplext, despite the very small datasets used for training and tuning.
  bibtex_file: /assets/bib/ranlp2015.bib
  bibtex: >
    @InProceedings{Stajner2015RANLP,
      author    = {\v{S}tajner, Sanja  and  Calixto, Iacer  and  Saggion, Horacio},
      title     = {Automatic Text Simplification for Spanish: Comparative Evaluation of Various Simplification Strategies},
      booktitle = {Proceedings of the International Conference Recent Advances in Natural Language Processing},
      month     = {September},
      year      = {2015},
      address   = {Hissar, Bulgaria},
      publisher = {INCOMA Ltd. Shoumen, BULGARIA},
      pages     = {618--626},
      url       = {http://www.aclweb.org/anthology/R15-1080}
    }
-
  priority: 10000
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2016
  img: wmt2016
  title: "DCU&ndash;UvA Multimodal MT System Report"
  authors: "<strong>Iacer Calixto</strong>, Desmond Elliott, Stella Frank"
  [//]: # booktitle: The First Conference on Machine Translation
  booktitle: WMT
  doc-url: http://www.statmt.org/wmt16/pdf/W16-2359.pdf
  venues: sharedtask,short
  abstract: >
    We present a doubly-attentive multimodal machine translation model. Our model learns to attend to source language and spatial-preserving CONV5,4 visual features as separate attention mechanisms in a neural translation model. In image description translation experiments (Task 1), we find an improvement of 2.3 Meteor points compared to initialising the hidden state of the decoder with only the FC7 features and 2.9 Meteor points compared to a text-only neural machine translation baseline, confirming the useful nature of attending to the CONV5,4 features.
  bibtex_file: /assets/bib/wmt2016.bib
  bibtex: >
    @InProceedings{Calixto2016WMT,
      author    = {Calixto, Iacer  and  Elliott, Desmond  and  Frank, Stella},
      title     = {DCU-UvA Multimodal MT System Report},
      booktitle = {Proceedings of the First Conference on Machine Translation},
      month     = {August},
      year      = {2016},
      address   = {Berlin, Germany},
      publisher = {Association for Computational Linguistics},
      pages     = {634--638},
      url       = {http://www.aclweb.org/anthology/W16-2359}
    }
-
  priority: 10000
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2016
  img: wmt2016a
  title: "Multimodal neural machine translation using minimum risk training"
  authors: "Chris Hokamp, <strong>Iacer Calixto</strong>"
  [//]: # booktitle: The First Conference on Machine Translation
  booktitle: WMT
  venues: sharedtask
  code: https://github.com/chrishokamp/multimodal_nmt
  bibtex_file: #
-
  priority: 10000
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2016
  img: lrec2016
  title: "Developing a Dataset for Evaluating Approaches for Document Expansion with Images"
  authors: "Debasis Ganguly, <strong>Iacer Calixto</strong>, Gareth Jones"
  [//]: # booktitle: Proceedings of the Tenth International Conference on Language Resources and Evaluation
  booktitle: LREC
  doc-url: https://www.aclweb.org/anthology/L16-1299.pdf
  venues: conference,short
  abstract: >
    Motivated by the adage that a “picture is worth a thousand words” it can be reasoned that automatically enriching the textual content of a document with relevant images can increase the readability of a document. Moreover, features extracted from the additional image data inserted into the textual content of a document may, in principle, be also be used by a retrieval engine to better match the topic of a document with that of a given query. In this paper, we describe our approach of building a ground truth dataset to enable further research into automatic addition of relevant images to text documents. The dataset is comprised of the official ImageCLEF 2010 collection (a collection of images with textual metadata) to serve as the images available for automatic enrichment of text, a set of 25 benchmark documents that are to be enriched, which in this case are children’s short stories, and a set of manually judged relevant images for each query story obtained by the standard procedure of depth pooling. We use this benchmark dataset to evaluate the effectiveness of standard information retrieval methods as simple baselines for this task. The results indicate that using the whole story as a weighted query, where the weight of each query term is its tf-idf value, achieves an precision of 0.1714 within the top 5 retrieved images on an average.
  bibtex_file: /assets/bib/lrec2016.bib
  bibtex: >
    @InProceedings{GANGULY2016LREC,
      author = {Debasis Ganguly and Iacer Calixto and Gareth Jones},
      title = {Developing a Dataset for Evaluating Approaches for Document Expansion with Images},
      booktitle = {Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)},
      year = {2016},
      month = {may},
      date = {23-28},
      location = {Portorož, Slovenia},
      editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Sara Goggi and Marko Grobelnik and Bente Maegaard and Joseph Mariani and Helene Mazo and Asuncion Moreno and Jan Odijk and Stelios Piperidis},
      publisher = {European Language Resources Association (ELRA)},
      address = {Paris, France},
      isbn = {978-2-9517408-9-1},
      language = {english}
     }
-
  priority: 100
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2017
  img: acl2017
  title: "Doubly-Attentive Decoder for Multi-modal Neural Machine Translation"
  authors: "<strong>Iacer Calixto</strong>, Qun Liu, Nick Campbell"
  doc-url: http://aclweb.org/anthology/P17-1175
  [//]: # booktitle: "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"
  booktitle: ACL
  venues: conference,long
  abstract: >
      We introduce a Multi-modal Neural Machine Translation model in which a doubly-attentive decoder naturally incorporates spatial visual features obtained using pre-trained convolutional neural networks, bridging the gap between image description and translation. Our decoder learns to attend to source-language words and parts of an image independently by means of two separate attention mechanisms as it generates words in the target language. We find that our model can efficiently exploit not just back-translated in-domain multi-modal data but also large general-domain text-only MT corpora. We also report state-of-the-art results on the Multi30k data set.
  bibtex_file: /assets/bib/acl2017.bib
  bibtex: >
    @InProceedings{CalixtoACL2017,
      author = 	    "Calixto, Iacer and Liu, Qun and Campbell, Nick",
      title = 	    "Doubly-Attentive Decoder for Multi-modal Neural Machine Translation",
      booktitle =   "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      year = 	    "2017",
      publisher =   "Association for Computational Linguistics",
      pages = 	    "1913--1924",
      location =    "Vancouver, Canada",
      doi = 	    "10.18653/v1/P17-1175",
      url = 	    "http://www.aclweb.org/anthology/P17-1175"
    }
-
  priority: 200
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2017
  img: emnlp2017
  title: "Incorporating Global Visual Features into Attention-Based Neural Machine Translation"
  authors: "<strong>Iacer Calixto</strong>, Qun Liu"
  doc-url: http://aclweb.org/anthology/D17-1105
  [//]: # booktitle: "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
  booktitle: EMNLP
  venues: conference,long
  abstract: >
      We   introduce   multi-modal, attention-based Neural Machine Translation (NMT) models which incorporate visual features into  different  parts  of  both  the  encoder and  the  decoder.   Global  image  features are  extracted  using  a  pre-trained  convolutional  neural  network  and  are  incorporated (i) as words in the source sentence, (ii) to  initialise  the  encoder  hidden  state, and (iii) as  additional  data  to  initialise the  decoder  hidden  state.   In  our  experiments,  we  evaluate  translations  into  English and German, how different strategies to incorporate global image features compare  and  which  ones  perform  best.   We also study the impact that adding synthetic multi-modal, multilingual data brings and find  that  the  additional  data  have  a  positive  impact  on  multi-modal  models.   We report new state-of-the-art results and our best models also significantly improve on a comparable Phrase-Based Statistical MT (PBSMT) model trained on the Multi30k data set according to all metrics evaluated. To the best of our knowledge, it is the first time  a  purely  neural  model  significantly improves over a PBSMT model on all metrics evaluated on this data set.
  bibtex_file: /assets/bib/emnlp2017.bib
  bibtex: >
    @InProceedings{calixto-liu:2017:EMNLP2017,
      author    = {Calixto, Iacer  and  Liu, Qun},
      title     = {Incorporating Global Visual Features into Attention-based Neural Machine Translation.},
      booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
      month     = {September},
      year      = {2017},
      address   = {Copenhagen, Denmark},
      publisher = {Association for Computational Linguistics},
      pages     = {992--1003},
      abstract  = {We introduce multi-modal, attention-based neural machine translation (NMT)
            models which incorporate visual features into different parts of both the
            encoder and the decoder. Global image features are extracted using a
            pre-trained convolutional neural network and are incorporated (i) as words in
            the source sentence, (ii) to initialise the encoder hidden state, and (iii) as
            additional data to initialise the decoder hidden state. In our experiments, we
            evaluate translations into English and German, how different strategies to
            incorporate global image features compare and which ones perform best. We also
            study the impact that adding synthetic multi-modal, multilingual data brings
            and find that the additional data have a positive impact on multi-modal NMT
            models. We report new state-of-the-art results and our best models also
            significantly improve on a comparable phrase-based Statistical MT (PBSMT) model
            trained on the Multi30k data set according to all metrics evaluated. To the
            best of our knowledge, it is the first time a purely neural model significantly
            improves over a PBSMT model on all metrics evaluated on this data set.},
      url       = {https://www.aclweb.org/anthology/D17-1105}
    }
-
  priority: 400
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2017
  img: eacl2017
  title: "Using Images to Improve Machine-Translating E-Commerce Product Listings"
  authors: "<strong>Iacer Calixto</strong>, Daniel Stein, Evgeny Matusov, Pintu Lohar, Sheila Castilho, Andy Way"
  doc-url: http://aclweb.org/anthology/E17-2101
  [//]: # booktitle: "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers"
  booktitle: EACL
  venues: conference,short
  abstract: >
      In this paper we study the impact of using images to machine-translate user-generated ecommerce product listings. We study how a multi-modal Neural Machine Translation (NMT) model compares to two text-only approaches: a conventional state-of-the-art attentional NMT and a Statistical Machine Translation (SMT) model. User-generated product listings often do not constitute grammatical or well-formed sentences. More often than not, they consist of the juxtaposition of short phrases or keywords. We train our models end-to-end as well as use text-only and multimodal NMT models for re-ranking n-best lists generated by an SMT model. We qualitatively evaluate our user-generated training data also analyse how adding synthetic data impacts the results. We evaluate our models quantitatively using BLEU and TER and find that (i) additional synthetic data has a general positive impact on text-only and multi-modal NMT models, and that (ii) using a multi-modal NMT model for re-ranking n-best lists improves TER significantly across different nbest list sizes.
  bibtex_file: /assets/bib/eacl2017.bib
  bibtex: >
    @InProceedings{Calixto2017EACL,
      author    = {Calixto, Iacer  and  Stein, Daniel  and  Matusov, Evgeny  and  Lohar, Pintu  and  Castilho, Sheila  and  Way, Andy},
      title     = {Using Images to Improve Machine-Translating E-Commerce Product Listings.},
      booktitle = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
      month     = {April},
      year      = {2017},
      address   = {Valencia, Spain},
      publisher = {Association for Computational Linguistics},
      pages     = {637--643},
      url       = {http://www.aclweb.org/anthology/E17-2101}
    }
-
  priority: 500
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2017
  img: ranlp2017
  title: "Sentence-Level Multilingual Multi-modal Embedding for Natural Language Processing"
  authors: "<strong>Iacer Calixto</strong>, Qun Liu"
  doc-url: http://www.acl-bg.org/proceedings/2017/RANLP%202017/pdf/RANLP020.pdf
  [//]: # booktitle: "Proceedings of Recent Advances in Natural Language Processing"
  booktitle: RANLP
  venues: conference,long
  abstract: >
      We propose a novel discriminative ranking model that learns embeddings from multilingual and multi-modal data, meaning that our model can take advantage of images and descriptions in multiple language to improve embedding quality. To that end, we introduce an objective function that uses pairwise ranking adapted to the case of three or more input sources. We compare our model against different baselines, and evaluate the robustness of our embeddings on image–sentence ranking (ISR), semantic textual similarity (STS), and neural machine translation (NMT). We find that the additional multilingual signals lead to improvements on all three tasks, and we highlight that our model can be used to consistently improve the adequacy of translations generated with NMT models when re-ranking n-best lists.
  bibtex_file: /assets/bib/ranlp2017.bib
  bibtex: >
    @InProceedings{Calixto2017RANLP,
      author    = {Calixto, Iacer and Liu, Qun},
      title     = {{Sentence-Level Multilingual Multi-modal Embedding for Natural Language Processing}},
      booktitle = {Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017},
      month     = {September},
      year      = {2017},
      address   = {Varna, Bulgaria},
      pages     = {139--148},
      url       = {https://doi.org/10.26615/978-954-452-049-6_020}
    }
-
  priority: 300
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2017
  img: vl2017
  title: "Human Evaluation of Multi-modal Neural Machine Translation: A Case-Study on E-Commerce Listing Titles"
  authors: "<strong>Iacer Calixto</strong>, Daniel Stein, Evgeny Matusov, Sheila Castilho, Andy Way"
  doc-url: http://www.aclweb.org/anthology/W17-2004
  booktitle: "6th Workshop on Vision and Language"
  venues: workshop,short
  abstract: >
      In this paper, we study how humans perceive the use of images as an additional knowledge source to machine-translate user-generated product listings in an e-commerce company. We conduct a human evaluation where we assess how a multi-modal neural machine translation (NMT) model compares to two text-only approaches: a conventional state-of-the-art attention-based NMT and a phrase-based statistical machine translation (PBSMT) model. We evaluate translations obtained with different systems and also discuss the data set of user-generated product listings, which in our case comprises both product listings and associated images. We found that humans preferred translations obtained with a PBSMT system to both text-only and multi-modal NMT over 56% of the time. Nonetheless, human evaluators ranked translations from a multi-modal NMT model as better than those of a text-only NMT over 88% of the time, which suggests that images do help NMT in this use-case.
  bibtex_file: /assets/bib/vl2017.bib
  bibtex: >
    @InProceedings{Calixto2017VL,
      author    = {Calixto, Iacer  and  Stein, Daniel  and  Matusov, Evgeny  and  Castilho, Sheila  and  Way, Andy},
      title     = {Human Evaluation of Multi-modal Neural Machine Translation: A Case-Study on E-Commerce Listing Titles},
      booktitle = {Proceedings of the Sixth Workshop on Vision and Language},
      month     = {April},
      year      = {2017},
      address   = {Valencia, Spain},
      publisher = {Association for Computational Linguistics},
      pages     = {31--37},
      url       = {http://www.aclweb.org/anthology/W17-2004}
    }
-
  priority: 150
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2017
  img: inlg2017
  title: "Linguistic realisation as machine translation: Comparing different MT models for AMR-to-text generation"
  authors: "Thiago Castro Ferreira, <strong>Iacer Calixto</strong>, Sander Wubben and Emiel Krahmer"
  doc-url: http://www.aclweb.org/anthology/W17-3501
  [//]: # booktitle: "Proceedings of The 10th International Natural Language Generation conference"
  booktitle: INLG
  venues: conference,long,bestpaper
  abstract: >
      In this paper, we study AMR-to-text generation, framing it as a translation task and comparing two different MT approaches (Phrase-based and Neural MT). We systematically study the effects of 3 AMR preprocessing steps (Delexicalisation, Compression, and Linearisation) applied before the MT phase. Our results show that preprocessing indeed helps, although the benefits differ for the two MT models. The implementation of the models are publicly available.
  bibtex_file: /assets/bib/inlg2017.bib
  bibtex: >
    @InProceedings{CastroFerreira2017INLG,
      author = 	"Castro Ferreira, Thiago and Calixto, Iacer and Wubben, Sander and Krahmer, Emiel",
      title = 	"Linguistic realisation as machine translation: Comparing different MT models for AMR-to-text generation",
      booktitle = 	"Proceedings of the 10th International Conference on Natural Language Generation",
      year = 	"2017",
      publisher = 	"Association for Computational Linguistics",
      pages = 	"1--10",
      location = 	"Santiago de Compostela, Spain",
      url = 	"http://aclweb.org/anthology/W17-3501"
    }
-
  priority: 1000
  layout: article
  paper-type: article
  selected: y
  year: 2017
  img: pbml2017
  title: "Is Neural Machine Translation the New State-of-the-Art?"
  authors: "Sheila Castilho, Joss Moorkens, Federico Gaspari, <strong>Iacer Calixto</strong>, John Tinsley, Andy Way"
  journal-url: https://ufal.mff.cuni.cz/pbml/108/art-castilho-moorkens-gaspari-tinsley-calixto-way.pdf
  [//]: # journal: "The Prague Bulletin of Mathematical Linguistics"
  journal: PBML
  venues: journal,long
  abstract: >
      This paper discusses neural machine translation (NMT), a new paradigm in the MT field, comparing the quality of NMT systems with statistical MT by describing three studies using automatic and human evaluation methods. Automatic evaluation results presented for NMT are very promising, however human evaluations show mixed results. We report increases in fluency but inconsistent results for adequacy and post-editing effort. NMT undoubtedly represents a step forward for the MT field, but one that the community should be careful not to oversell.
  bibtex_file: /assets/bib/pbml2017.bib
  bibtex: >
    @article{castilho-moorkens-gaspari-tinsley-calixto-way:2017,
     journal = {The Prague Bulletin of Mathematical Linguistics},
     title = {{Is Neural Machine Translation the New State of the Art?}},
     author = {Sheila Castilho and Joss Moorkens and Federico Gaspari and Iacer Calixto and John Tinsley and Andy Way},
     year = {2017},
     month = {June},
     volume = {108},
     pages = {109--120},
     doi = {10.1515/pralin-2017-0013},
     issn = {0032-6585},
     url = {https://ufal.mff.cuni.cz/pbml/108/art-castilho-moorkens-gaspari-tinsley-calixto-way.pdf}
    }
-
  priority: 100
  layout: article
  paper-type: article
  selected: y
  year: 2019
  img: mtjournal-2019
  title: "An Error Analysis for Image-Based Multi-Modal Neural Machine Translation"
  authors: "<strong>Iacer Calixto</strong>, Qun Liu"
  journal: "Machine Translation: Special Issue in Human Factors in Neural Machine Translation"
  journal-url: "https://link.springer.com/article/10.1007/s10590-019-09226-9"
  venues: journal,long
  abstract: >
    In this article, we conduct an extensive qualitative error analysis of different multi-modal neural machine translation (MNMT) models which integrate visual features into different parts of both the encoder and the decoder.
    We investigate how different training data availability scenarios impact different models, and analyse translations from German into English: in a first scenario, (i) there is only a small training data set of parallel sentence pairs with images, or (ii) in addition to that there are additional image descriptions in the target language with images, which are incorporated with back-translation (Sennrich et al., 2016).
    We analyse two different types of MNMT models, that use global and local image features: the latter encode an image globally, i.e. there is one feature vector computed for an entire image, whereas the former encode spatial information, i.e. there are multiple feature vectors, each encoding different portions of the image.
    We conduct an extensive error analysis of translations generated by different multi-modal NMT models as well as text-only baselines, where we study how multi-modal models compare when translating both visual and non-visual terms.
    In general, we find that the additional multi-modal signals consistently improve translations, even more so when using simpler multi-modal NMT models that use global visual features, and also that not only translations of terms with a strong visual connotation are improved, but almost all kinds of errors decreased by using (some) multi-modal models and more training data.
  bibtex_file: /assets/bib/mtjournal2019.bib
  bibtex: >
    @article{DBLP:journals/mt/CalixtoL19,
      author    = {Iacer Calixto and
                   Qun Liu},
      title     = {An error analysis for image-based multi-modal neural machine translation},
      journal   = {Mach. Transl.},
      volume    = {33},
      number    = {1-2},
      pages     = {155--177},
      year      = {2019},
      url       = {https://doi.org/10.1007/s10590-019-09226-9},
      doi       = {10.1007/s10590-019-09226-9},
      timestamp = {Sat, 30 May 2020 19:53:16 +0200},
      biburl    = {https://dblp.org/rec/journals/mt/CalixtoL19.bib},
      bibsource = {dblp computer science bibliography, https://dblp.org}
    }
-
  priority: 100
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2019
  img: latent-variable-model-for-multimodal-machine-translation-2019
  title: "Latent Variable Model for Multi-modal Translation"
  authors: "<strong>Iacer Calixto</strong>, Miguel Rios, Wilker Aziz"
  doc-url: https://www.aclweb.org/anthology/P19-1642/
  [//]: # booktitle: "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"
  booktitle: ACL
  venues: conference,long
  abstract: >
    In this work, we propose to model the interaction between visual and textual features for multi-modal neural machine translation through a latent variable model.
    This latent variable can be seen as a stochastic embedding and it is used in the target-language decoder and also to predict image features.
    Importantly, even though in our model formulation we capture correlations between visual and textual features, we do not require that images be available at test time.
    We show that our latent variable MMT formulation improves considerably over strong baselines, including the multi-task learning approach of Elliott and Kadar (2017) and the conditional variational auto-encoder approach of Toyama et al. (2016).
    Finally, in an ablation study we show that (i) predicting image features in addition to only conditioning on them and (ii) imposing a constraint on the minimum amount of information encoded in the latent variable slightly improved translations.
  bibtex_file: /assets/bib/acl2019.bib
  bibtex: >
    @inproceedings{calixto-etal-2019-latent,
      title = "Latent Variable Model for Multi-modal Translation",
      author = "Calixto, Iacer  and
        Rios, Miguel  and
        Aziz, Wilker",
      booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      month = jul,
      year = "2019",
      address = "Florence, Italy",
      publisher = "Association for Computational Linguistics",
      url = "https://www.aclweb.org/anthology/P19-1642",
      doi = "10.18653/v1/P19-1642",
      pages = "6392--6405",
    }
-
  priority: 500
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2021
  img: seeing-past-words-2021
  title: "Seeing past words: Testing the cross-modal capabilities of pretrained V&L models on counting tasks"
  authors: "Letitia Parcalabescu, Albert Gatt, Anette Frank, <strong>Iacer Calixto</strong>"
  doc-url: https://arxiv.org/abs/2012.12352
  booktitle: "Beyond Language: Multimodal Semantic Representations Workshop (MMSR)"
  venues: workshop,long
  abstract: >
    e investigate the reasoning ability of pretrained vision and language (V&L) models in two tasks that require multimodal integration: (1) discriminating a correct image-sentence pair from an incorrect one, and (2) counting entities in an image. We evaluate three pretrained V&L models on these tasks: ViLBERT, ViLBERT 12-in-1 and LXMERT, in zero-shot and finetuned settings. Our results show that models solve task (1) very well, as expected, since all models are pretrained on task (1). However, none of the pretrained V&L models is able to adequately solve task (2), our counting probe, and they cannot generalise to out-of-distribution quantities. We propose a number of explanations for these findings: LXMERT (and to some extent ViLBERT 12-in-1) show some evidence of catastrophic forgetting on task (1). Concerning our results on the counting probe, we find evidence that all models are impacted by dataset bias, and also fail to individuate entities in the visual input. While a selling point of pretrained V&L models is their ability to solve complex tasks, our findings suggest that understanding their reasoning and grounding capabilities requires more targeted investigations on specific phenomena.
  bibtex_file: /assets/bib/mmsr2021seeing.bib
-
  priority: 100
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2020
  img: aacl2020
  title: "English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too"
  authors: Jason Phang \*, <strong>Iacer Calixto</strong> \*, Phu Mon Htut, Yada Pruksachatkun, Haokun Liu, Clara Vania, Katharina Kann, Samuel R. Bowman. (\* Equal Contribution)
  doc-url: https://www.aclweb.org/anthology/2020.aacl-main.56/
  booktitle: "AACL"
  bibtex_file: /assets/bib/aacl2020.bib
  venues: conference,long
  abstract: Intermediate-task training---fine-tuning a pretrained model on an intermediate task before fine-tuning again on the target task---often improves model performance substantially on language understanding tasks in monolingual English settings. We investigate whether English intermediate-task training is still helpful on non-English target tasks. Using nine intermediate language-understanding tasks, we evaluate intermediate-task transfer in a zero-shot cross-lingual setting on the XTREME benchmark. We see large improvements from intermediate training on the BUCC and Tatoeba sentence retrieval tasks and moderate improvements on question-answering target tasks. MNLI, SQuAD and HellaSwag achieve the best overall results as intermediate tasks, while multi-task intermediate offers small additional improvements. Using our best intermediate-task models for each target task, we obtain a 5.4 point improvement over XLM-R Large on the XTREME benchmark, setting the state of the art as of June 2020. We also investigate continuing multilingual MLM during intermediate-task training and using machine-translated intermediate-task data, but neither consistently outperforms simply performing English intermediate-task training.
  bibtex: >
    @inproceedings{phang-etal-2020-english,
    title = "{E}nglish Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too",
    author = "Phang, Jason  and
      Calixto, Iacer  and
      Htut, Phu Mon  and
      Pruksachatkun, Yada  and
      Liu, Haokun  and
      Vania, Clara  and
      Kann, Katharina  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.aacl-main.56",
    pages = "557--575",
    abstract = "Intermediate-task training---fine-tuning a pretrained model on an intermediate task before fine-tuning again on the target task---often improves model performance substantially on language understanding tasks in monolingual English settings. We investigate whether English intermediate-task training is still helpful on non-English target tasks. Using nine intermediate language-understanding tasks, we evaluate intermediate-task transfer in a zero-shot cross-lingual setting on the XTREME benchmark. We see large improvements from intermediate training on the BUCC and Tatoeba sentence retrieval tasks and moderate improvements on question-answering target tasks. MNLI, SQuAD and HellaSwag achieve the best overall results as intermediate tasks, while multi-task intermediate offers small additional improvements. Using our best intermediate-task models for each target task, we obtain a 5.4 point improvement over XLM-R Large on the XTREME benchmark, setting the state of the art as of June 2020. We also investigate continuing multilingual MLM during intermediate-task training and using machine-translated intermediate-task data, but neither consistently outperforms simply performing English intermediate-task training.",
    }
-
  priority: 100
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2020
  img: aacl2020a
  title: "Are scene graphs good enough to improve Image Captioning?"
  authors: "Victor Milewski, Marie-Francine Moens, <strong>Iacer Calixto</strong>"
  doc-url: https://www.aclweb.org/anthology/2020.aacl-main.50/
  booktitle: "AACL"
  venues: conference,long
  abstract: >
    Many top-performing image captioning models rely solely on object features computed with an object detection model to generate image descriptions. However, recent studies propose to directly use scene graphs to introduce information about object relations into captioning, hoping to better describe interactions between objects. In this work, we thoroughly investigate the use of scene graphs in image captioning. We empirically study whether using additional scene graph encoders can lead to better image descriptions and propose a conditional graph attention network (C-GAT), where the image captioning decoder state is used to condition the graph updates. Finally, we determine to what extent noise in the predicted scene graphs influence caption quality. Overall, we find no significant difference between models that use scene graph features and models that only use object detection features across different captioning metrics, which suggests that existing scene graph generation models are still too noisy to be useful in image captioning. Moreover, although the quality of predicted scene graphs is very low in general, when using high quality scene graphs we obtain gains of up to 3.3 CIDEr compared to a strong Bottom-Up Top-Down baseline.
  bibtex_file: /assets/bib/aacl2020a.bib
-
  priority: 1000
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2020
  img: winlp-2020
  title: "Can Wikipedia Categories Improve Masked Language Model Pretraining?"
  venues: workshop,short
  authors: "Diksha Meghwal, Katharina Kann, <strong>Iacer Calixto</strong>, Stanislaw Jastrzebski"
  doc-url: https://www.aclweb.org/anthology/2020.winlp-1.19/
  booktitle: "Widening NLP Workshop"
  bibtex_file: /assets/bib/winlp2020.bib
-
  priority: 600
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2021
  img: visualsem-2021
  title: "VisualSem: a high-quality knowledge graph for vision and language"
  authors: "Houda Alberts, Teresa Huang, Yash Deshpande, Yibo Liu, Kyunghyun Cho, Clara Vania, <strong>Iacer Calixto</strong>"
  doc-url: https://arxiv.org/abs/2008.09150
  booktitle: "Multilingual Representation Learning Workshop"
  venues: workshop,long
  abstract: >
    An exciting frontier in natural language understanding (NLU) and generation (NLG) calls for (vision-and-) language models that can efficiently access external structured knowledge repositories. However, many existing knowledge bases only cover limited domains, or suffer from noisy data, and most of all are typically hard to integrate into neural language pipelines. To fill this gap, we release VisualSem: a high-quality knowledge graph (KG) which includes nodes with multilingual glosses, multiple illustrative images, and visually relevant relations. We also release a neural multi-modal retrieval model that can use images or sentences as inputs and retrieves entities in the KG. This multi-modal retrieval model can be integrated into any (neural network) model pipeline. We encourage the research community to use VisualSem for data augmentation and/or as a source of grounding, among other possible uses. VisualSem as well as the multi-modal retrieval models are publicly available and can be downloaded in this URL: https://github.com/iacercalixto/visualsem.
  bibtex_file: /assets/bib/visualsem_dataset.bib
-
  priority: 700
  layout: paper
  paper-type: article
  selected: n
  year: 2021
  img: visualsem-representation-learning-2021
  title: "Learning Robust Multimodal Knowledge Graph Representations"
  authors: "Ningyuan (Teresa) Huang, Yash Deshpande, Yibo Liu, Houda Alberts, Kyunghyun Cho, Clara Vania, <strong>Iacer Calixto</strong>"
  doc-url: 
  journal: "(Under Review)"
  venues: notpublic,long
  abstract: >
    We propose a new method to make natural language understanding models more parameter efficient by storing knowledge in an external knowledge graph (KG) and retrieving from this KG using a dense index. Given some task data, e.g., sentences in German, we retrieve entities from the KG and use their multimodal representations to improve downstream task performance. Using VisualSem as our KG, we compare a mix of tuple-based and graph-based algorithms to learn robust representations of entities that are grounded on their multimodal information. We then demonstrate the usefulness of our learned entity representations on two downstream tasks. Using our best learned representations, we improve performance on the multilingual named entity recognition (NER) task by 0.3%-0.7% (F1 score), while on the visual sense disambiguation task, we achieve up to 3% improvement in accuracy in the low-resource setting.
  bibtex_file: #
-
  priority: 800
  layout: incollection
  paper-type: incollection
  selected: y
  year: 2021
  img: nlp4health-2021
  title: "Natural language processing for mental disorders: an overview"
  authors: "<strong>Iacer Calixto</strong>, Victoria Yaneva, Raphael Moura Cardoso"
  booktitle: "Natural Language Processing in Healthcare: A Special Focus on Low Resource Languages"
  publisher: "CRC Press"
  venues: chapter,long
  abstract: >
    In recent years, there has been a surge in interest in using natural language processing (NLP) applications for clinical psychology and psychiatry. Despite the increased societal, economic, and academic interest, there has been no systematic critical analysis of the recent progress in NLP applications for mental disorders, or of the resources available for training and evaluating such systems.

    This chapter addresses this gap through two main contributions. First, it provides an overview of the NLP literature related to mental disorders, with a focus on autism, dyslexia, schizophrenia, depression and mental health in general. We discuss the strengths and shortcomings of current methodologies, specifically focusing on the challenges in obtaining large volumes of high-quality domain-specific data both for English and for lower-resource languages. We also provide a list of datasets publicly available for researchers who would like to develop NLP methods for specific mental disorders, categorized according to relevant criteria such as data source, language, annotation, and size. Our second contribution is a discussion on how to support the application of these methods to various languages and social contexts. This includes recommendations on conducting robust and ethical experiments from a machine learning perspective, and a discussion on how techniques such as cross-lingual transfer learning could be applied within this area.
  bibtex_file: /assets/bib/nlp4health2021.bib
  bibtex: >
    @incollection{calixtoetal2021mentalhealth,
      author    = {Iacer Calixto and Victoria Yaneva and Raphael Moura Cardoso},
      title     = {Natural language processing for mental disorders: an overview},
      booktitle = {Natural Language Processing in Healthcare: A Special Focus on Low Resource Languages},
      year      = {2021},
      publisher = {CRC Press},
    }

